# embeddings/build_vector_db.py

import os
import requests
import fitz
import xml.etree.ElementTree as ET
from glob import glob
from dotenv import load_dotenv
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import PyPDFLoader

# 1. í™˜ê²½ì„¤ì •
load_dotenv()
NONGSARO_API_KEY = os.getenv("NONGSARO_API_KEY")

project_root = os.path.dirname(os.path.abspath(__file__))
docs_dir = os.path.join(project_root, "docs")
downloads_dir = os.path.join(project_root, "downloads")
os.makedirs(downloads_dir, exist_ok=True)

# 2. ê¸°ì¡´ PDF ë¡œë“œ (íŠ¹ì • í˜ì´ì§€ ë²”ìœ„ë§Œ)
all_docs = []
for path in glob(os.path.join(docs_dir, "*.pdf")):
    filename = os.path.basename(path)
    loader = PyPDFLoader(path)
    pages = loader.load()
    
    # docsì— ìˆëŠ” ë”¸ê¸°,ì°¸ì™¸.pdf ì‚¬ìš©
    for page in pages:
        page.metadata["source"] = filename
        all_docs.append(page)
    
    print(f"ğŸ“„ {filename}: {len(pages)}í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ")
    

# 3. ë†ì§„ì²­ ë³‘í•´ì¶© PDF â†’ Document ë³€í™˜
def download_file(url, filename):
    res = requests.get(url)
    if res.status_code == 200:
        path = os.path.join(downloads_dir, filename)
        with open(path, "wb") as f:
            f.write(res.content)
        return path
    return None

def extract_text_from_pdf(path):
    doc = fitz.open(path)
    return "\n".join(page.get_text() for page in doc)

def get_nongsaro_documents(keywords, year="2024"):
    docs = []
    url = "http://api.nongsaro.go.kr/service/dbyhsCccrrncInfo/dbyhsCccrrncInfoList"
    params = {"apiKey": NONGSARO_API_KEY, "sYear": year}
    res = requests.get(url, params=params)
    if res.status_code != 200:
        return docs
    root = ET.fromstring(res.content)
    items = root.findall(".//item")
    for item in items:
        title = item.findtext("cntntsSj")
        file_url = item.findtext("downFile")
        file_name = item.findtext("rtnOrginlFileNm")
        if file_url and file_name:
            path = download_file(file_url, file_name)
            if path:
                text = extract_text_from_pdf(path)
                for kw in keywords:
                    if kw in text:
                        docs.append(Document(
                            page_content=text,
                            metadata={"source": f"NongSaRo_{file_name}", "matched_keyword": kw}
                        ))
                        break
    return docs

# 4. ë³‘í•´ì¶© PDF ë¡œë“œ ë° ë³‘í•©
keywords = ["ë”¸ê¸°", "ì°¸ì™¸", "ë‹¨í˜¸ë°•"]
nongsaro_docs = get_nongsaro_documents(keywords)
all_docs += nongsaro_docs

# 5. ë¬¸ì„œ ì²­í¬ ë° ë²¡í„°í™”
splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)
chunks = splitter.split_documents(all_docs)

texts = [doc.page_content for doc in chunks]
metadatas = [doc.metadata for doc in chunks]

# HuggingFace ì„ë² ë”© ì‚¬ìš©
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

batch_size = 50
all_embs = []
for i in range(0, len(texts), batch_size):
    batch = texts[i : i + batch_size]
    embs = embeddings.embed_documents(batch)
    all_embs.extend(embs)

# 6. FAISS ì €ì¥
vectorstore = FAISS.from_embeddings(
    text_embeddings=list(zip(texts, all_embs)),
    embedding=embeddings,
    metadatas=metadatas
)
parent_dir = os.path.dirname(project_root)
index_path = os.path.join(parent_dir, "embeddings", "faiss_index")
os.makedirs(index_path, exist_ok=True)
vectorstore.save_local(index_path)

print(f"âœ… ì´ {len(chunks)}ê°œ ë¬¸ì„œë¥¼ ì¸ë±ì‹±í•˜ì—¬ ì €ì¥ ì™„ë£Œ: {index_path}")
