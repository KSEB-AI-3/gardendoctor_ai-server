import os
from dotenv import load_dotenv

from langchain.agents import initialize_agent, Tool
from langchain.agents.agent_types import AgentType
from langchain_tavily import TavilySearch
from langchain.chains import RetrievalQA, LLMChain
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

from langgraph.graph import END, StateGraph
from pydantic import BaseModel
from typing import TypedDict

# 1. í™˜ê²½ ë³€ìˆ˜
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")

llm = ChatOpenAI(model_name="gpt-3.5-turbo", openai_api_key=OPENAI_API_KEY)
print(f"[LangGraph Agent] ì‚¬ìš© ëª¨ë¸: {llm.model_name}")

# 2. íˆ´ ì •ì˜
# direct_answer íˆ´ (gpt ì§ì ‘ ì‘ë‹µ)
direct_prompt = PromptTemplate(input_variables=["query"], template="{query}")
direct_chain = direct_prompt | llm  # RunnableSequence

direct_tool = Tool(
    name="direct_answer",
    func=lambda q: direct_chain.invoke({"query": q}),
    description="ì‘ë¬¼ ì¬ë°°ë‚˜ ìƒì‹ ì§ˆë¬¸ì— ëŒ€í•´ ì§ì ‘ ì‘ë‹µí•©ë‹ˆë‹¤. ì˜ˆ: 'ë”¸ê¸° ìˆ˜í™• ì‹œê¸° ì•Œë ¤ì¤˜', 'ì‘ë¬¼ì˜ ê´‘í•©ì„± ì¡°ê±´ì€?'"
)

# knowledge_db íˆ´ (ë²¡í„° DB + Fallback)
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vectorstore = FAISS.load_local("embeddings/faiss_index", embeddings, allow_dangerous_deserialization=True)
retriever = vectorstore.as_retriever()
knowledge_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)

def knowledge_with_fallback(query: str) -> str:
    """ë²¡í„° DB ê²€ìƒ‰ í›„ ê²°ê³¼ê°€ ë¶€ì¡±í•˜ë©´ GPTë¡œ fallback"""
    try:
        response = knowledge_chain.run(query)
        print(f"[DEBUG] ë²¡í„° DB ì‘ë‹µ: {response[:100]}...")  # ë””ë²„ê¹…ìš©
        
        fallback_phrases = [
           "ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤",
            "ì •í™•í•œ ë‹µë³€ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤", 
            "ê´€ë ¨ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤",
            "í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•œ",
            "ì£„ì†¡í•©ë‹ˆë‹¤",
            "ë‚´ìš©ì´ ì—†ë„¤ìš”",  
            "ì •ë³´ì—", 
            "ê°€ì ¸ì˜¨ ì •ë³´ì—"
        ]
        
        # ë¶ˆì¶©ë¶„í•œ ë‹µë³€ì´ë©´ GPTë¡œ ì§ì ‘ ë‹µë³€
        if any(phrase in response for phrase in fallback_phrases) or len(response.strip()) < 50:
            print(f"[fallback] knowledge_db ì‘ë‹µ ë¶€ì¡± â†’ direct_answer ì¬ì‹œë„: {query}")
            
            # ë†ì—… ì „ë¬¸ í”„ë¡¬í”„íŠ¸ë¡œ GPT ë‹µë³€ ìƒì„±
            agricultural_prompt = f"""
ë†ì—… ì „ë¬¸ê°€ë¡œì„œ ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ìƒì„¸í•˜ê³  ì‹¤ìš©ì ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {query}

ë‹µë³€ ê°€ì´ë“œë¼ì¸:
1. ë†ì—… ì „ë¬¸ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ êµ¬ì²´ì ì¸ ì •ë³´ ì œê³µ
2. ì¬ë°° ë°©ë²•, ê´€ë¦¬ ìš”ë ¹, ì£¼ì˜ì‚¬í•­ ë“± ì‹¤í–‰ ê°€ëŠ¥í•œ ì¡°ì–¸
3. ë³‘í•´ì¶© ê´€ë ¨ì´ë©´ ì¦ìƒ, ì›ì¸, ë°©ì œ ë°©ë²• í¬í•¨
4. ì „ë¬¸ìš©ì–´ëŠ” ì‰½ê²Œ í’€ì–´ì„œ ì„¤ëª…
5. ê³„ì ˆë³„ ê´€ë¦¬ íŒì´ë‚˜ ì¶”ê°€ ì •ë³´ê°€ ìˆë‹¤ë©´ í¬í•¨

ë‹µë³€:"""
            
            gpt_result = direct_chain.invoke({"query": agricultural_prompt})
            final_answer = gpt_result.content if hasattr(gpt_result, 'content') else str(gpt_result)
            print(f"âœ… [FALLBACK ì™„ë£Œ] GPT ë‹µë³€ ìƒì„±ë¨")
            return f"{final_answer}\n\nğŸ’¡ *ë†ì—… ì „ë¬¸ AI ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ë“œë ¸ìŠµë‹ˆë‹¤.*"
        print(f"âœ… [ë²¡í„° DB] ì¶©ë¶„í•œ ì‘ë‹µ ë°˜í™˜")
        return response
        
    except Exception as e:
        print(f"[ERROR] knowledge_db ì˜¤ë¥˜ â†’ direct_answerë¡œ fallback: {e}")
        fallback_prompt = f"ë†ì—… ì „ë¬¸ê°€ë¡œì„œ '{query}'ì— ëŒ€í•´ ë„ì›€ì´ ë˜ëŠ” ì •ë³´ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”."
        fallback_result = direct_chain.invoke({"query": fallback_prompt})
        return fallback_result.content if hasattr(fallback_result, 'content') else str(fallback_result)

# knowledge_db íˆ´
knowledge_tool = Tool(
    name="knowledge_db",
    func=knowledge_with_fallback,
    description=(
        "ì „ë¬¸ PDF ë¬¸ì„œì™€ ë†ì‚¬ë¡œ ë°ì´í„°ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì•„ ìƒì„¸íˆ ìš”ì•½í•©ë‹ˆë‹¤. "
        "ë²¡í„° DBì— ì¶©ë¶„í•œ ì •ë³´ê°€ ì—†ìœ¼ë©´ ë†ì—… ì „ë¬¸ AI ì§€ì‹ìœ¼ë¡œ ìë™ ë³´ì™„í•©ë‹ˆë‹¤. "
        "ì‘ë¬¼ ì¬ë°°, ë³‘í•´ì¶© ë°©ì œ, ë†ì—… ê¸°ìˆ  ë“± ëª¨ë“  ë†ì—… ê´€ë ¨ ì§ˆë¬¸ì— ëŒ€ì‘í•©ë‹ˆë‹¤. "
        "ì˜ˆ: 'ë”¸ê¸° ìì— í° ë°˜ì  ìƒê²¼ì–´', 'í† ë§ˆí†  ì¬ë°°ë²•', 'ì‘ë¬¼ ë³‘í•´ì¶© ì›ì¸ê³¼ ëŒ€ì‘ ë°©ë²•'"
    )
)
# web_search íˆ´
tavily = TavilySearch(api_key=TAVILY_API_KEY)
web_tool = Tool(name="web_search", func=tavily.run, description=
                "ì‹¤ì‹œê°„ ì •ë³´(ë‚ ì”¨, í˜„ì¬ ê¸°ì˜¨, ë‰´ìŠ¤ ë“±)ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤. "
                "ì˜ˆ: 'ì˜¤ëŠ˜ ë‚ ì”¨ ì–´ë•Œ?', 'ì§€ê¸ˆ ê¸°ì˜¨ì´ ëª‡ ë„ì•¼?', 'ê²½ê¸°ë„ ì§€ì—­ ê°•ìˆ˜ëŸ‰ ì˜ˆë³´ ì•Œë ¤ì¤˜'")

tools = [direct_tool, knowledge_tool, web_tool]

# 3. LangChain ReAct ê¸°ë°˜ Agent êµ¬ì„±
agent = initialize_agent(
    tools,
    llm,
    agent=AgentType.OPENAI_FUNCTIONS,  # ReAct ê¸°ë°˜ ì—ì´ì „íŠ¸
    verbose=True
)

# 4. LangGraphìš© ìƒíƒœ ëª¨ë¸
class AgentState(TypedDict):
    input: str
    output: str

# 5. LangGraph ë…¸ë“œ ì •ì˜
def run_agent_step(state: AgentState):
    user_input = state["input"]
    response = agent.run(user_input)
    return {"output": response}

# 6. LangGraph ê·¸ë˜í”„ êµ¬ì„±
builder = StateGraph(AgentState)
builder.set_entry_point("agent_executor")
builder.add_node("agent_executor", run_agent_step)
builder.add_edge("agent_executor", END)

agent_graph = builder.compile()

# 7. FastAPIìš© í•¨ìˆ˜
async def run_agent(query: str) -> str:
    result = await agent_graph.ainvoke({"input": query})
    return f"{result['output']}"

